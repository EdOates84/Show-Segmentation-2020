{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiD-2967SiVY",
        "colab_type": "text"
      },
      "source": [
        "# **Get Anchors**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fciIufBSJCp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "14f8e28e-89c4-42b7-cec1-0b7035f4dfea"
      },
      "source": [
        "pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=6056db1015616bd45caf880e0c65fe31bc6656691f1152054b9bce99d944d19e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6aYnCnSS3SB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikipedia as wiki\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "GENRE_TYPES = 'Talk-Show|News'\n",
        "TITLE_TYPE = 'tvSeries'\n",
        "PROFESSION_TYPES =  [' host',' presenter',' journalist',' news anchor','correspondent',' anchor']\n",
        "SPLIT_TYPES = [' is a ',' is an ',' was a ',' was an ',' is the ']\n",
        "# IMDB_PROFESSION_TYPES = 'actor|actress|miscellaneous|Null'\n",
        "NONE = 'Not Found'\n",
        "PAGE_ERROR = 'does not match any pages'\n",
        "DISAMBIGUATION_ERROR = 'disambiguation'\n",
        "NETWORK_TYPES = ['FOX','ABC','CBS','NBC','CNN','United Paramount Net','Warner Bros.','Pure Independent','PBS','Pax TV','Telemundo']\n",
        "REGION_TYPE = 'US'\n",
        "SHOW_NOT_IN_IMDB = 'Show is not present in IMDb'\n",
        "NAME_BASICS = \"/content/drive/My Drive/Datasets/name.basics.tsv/data.tsv\"\n",
        "TITLE_BASICS = \"/content/drive/My Drive/Datasets/title.basics.tsv/data.tsv\"\n",
        "TITLE_PRINCIPALS = \"/content/drive/My Drive/Datasets/title.principals.tsv.gz\"\n",
        "TITLE_AKAS = \"/content/drive/My Drive/Datasets/title.akas.tsv/data.tsv\"\n",
        "TITLE_CREW = \"/content/drive/My Drive/Datasets/title.crew.tsv/data.tsv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6IB1AiiS7Td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" After getting all professions of a person we can easily confirm that\n",
        "    our previous top 5 prediction is right or not & by using this we find \n",
        "    correct anchor.\n",
        "\"\"\"       \n",
        "\n",
        "def get_wiki_professions(Anchor_Name):\n",
        "  if len(wiki.search(Anchor_Name)) == 0:\n",
        "    return NONE\n",
        "\n",
        "  Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "  Store_name = Read_Namebasics.primaryName.tolist()\n",
        "\n",
        "  try:\n",
        "    if Anchor_Name in Store_name:\n",
        "      anchor_content = (((wiki.page(Anchor_Name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. '))\n",
        "    else:\n",
        "      return NONE\n",
        "  except wiki.exceptions.PageError as p:\n",
        "    if PAGE_ERROR in str(p):\n",
        "      return NONE\n",
        "  except wiki.exceptions.DisambiguationError as e:\n",
        "    Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "    Store_name = Read_Namebasics.primaryName.tolist()\n",
        "    if Anchor_Name in Store_name:\n",
        "      # print(e.options)\n",
        "      list_disambiguation_error = e.options\n",
        "      for i in list_disambiguation_error:\n",
        "        if i.find(DISAMBIGUATION_ERROR)!= -1:\n",
        "          return NONE\n",
        "        # print(i)\n",
        "        anchor_content = (((wiki.page(i,auto_suggest=False)).content).replace(' and ',', ').replace('.','. '))\n",
        "        # print(anchor_content)\n",
        "        \n",
        "        split_type = SPLIT_TYPES\n",
        "\n",
        "        split_type_list = []\n",
        "        prefix = None\n",
        "        for x in split_type:\n",
        "          if x in anchor_content:\n",
        "            prefix = x\n",
        "            split_type_index = anchor_content.find(prefix)\n",
        "            #  print(index_split_type)\n",
        "            pro = split_type_list.append(split_type_index)\n",
        "          else:\n",
        "            split_type_list.append(1000)\n",
        "\n",
        "        low_index = split_type[split_type_list.index(min(split_type_list))]\n",
        "        #  print(low_index)   \n",
        "            \n",
        "\n",
        "        if prefix is None:\n",
        "          return NONE\n",
        "\n",
        "        final_split = anchor_content.split(low_index)[1].split('. ')[0].split(', ')\n",
        "        # print(final_split)\n",
        "        final_professions = [anchor_content.lower() for anchor_content in final_split]\n",
        "        # return final_professions\n",
        "        # print(final_professions)\n",
        "        required_professions = PROFESSION_TYPES\n",
        "\n",
        "        check_individual_prof = []\n",
        "        for x in required_professions:\n",
        "          final_check= [(x in y) for y in final_professions]\n",
        "          check_individual_prof.append(any(final_check))\n",
        "          # print(x)\n",
        "        \n",
        "        if (any(check_individual_prof)==True):\n",
        "          return final_professions\n",
        "      else:\n",
        "        return NONE\n",
        "    \n",
        "    else:\n",
        "      return NONE     \n",
        "\n",
        "  # anchor_content = (((wiki.page(name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. '))\n",
        "  #  print(anchor_content)\n",
        "  split_type = SPLIT_TYPES\n",
        "  \n",
        "  split_type_list = []\n",
        "  prefix = None\n",
        "  for x in split_type:\n",
        "    if x in anchor_content:\n",
        "      prefix = x\n",
        "      split_type_index = anchor_content.find(prefix)\n",
        "      #  print(index_split_type)\n",
        "      pro = split_type_list.append(split_type_index)\n",
        "    else:\n",
        "      split_type_list.append(1000)\n",
        "\n",
        "  low_index = split_type[split_type_list.index(min(split_type_list))]\n",
        "  #  print(low_index)   \n",
        "      \n",
        "\n",
        "  if prefix is None:\n",
        "    return NONE\n",
        "\n",
        "  final_split = anchor_content.split(low_index)[1].split('. ')[0].split(', ')\n",
        "  # print(final_split)\n",
        "  final_professions = [anchor_content.lower() for anchor_content in final_split]\n",
        "  return final_professions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCVtyi0MGgI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "19efde2d-810c-411b-d902-2ea22247576d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIAvUzfjTET4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_wiki_professions(list_profession):\n",
        "  required_professions = PROFESSION_TYPES\n",
        "\n",
        "  check_individual_prof = []\n",
        "  for x in required_professions:\n",
        "    final_check= [(x in y) for y in list_profession]\n",
        "    check_individual_prof.append(any(final_check))\n",
        "    # print(final_check)\n",
        "    # print(x)\n",
        "  \n",
        "  if (any(check_individual_prof)==True):\n",
        "    # print(check_individual_prof)\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knfdTMUGTF_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"  From top 5 anchor we get most probable anchor in that show\n",
        "     Make sure that the input anchors list follows given format. Here's an example\n",
        "     Anchor_list = \"Colleen-Williams_Ashwini-Bhave_Elaine-Quijano_Becky-Hobbs_Heidi-Collins\"\n",
        "\"\"\"\n",
        "def get_correct_anchor(top_5):\n",
        "  top_5_list = (top_5.replace('-',' ').replace('_',',')).split(',')\n",
        "  # print(top_5_list)\n",
        "  top_5_list = list(filter(None, top_5_list))\n",
        "  for x in top_5_list:\n",
        "    # print(x)\n",
        "    # if Anchor_Name in Store_name:\n",
        "    # pqe = check_wiki_professions(get_wiki_professions(x)) is True:\n",
        "\n",
        "    if check_wiki_professions(get_wiki_professions(x)) is True:\n",
        "      Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "      Store_name = Read_Namebasics.primaryName.tolist()\n",
        "      if x in Store_name:\n",
        "        return x \n",
        "\n",
        "    # if pqe is True:\n",
        "      # list_of_selected_anchor.append(x)\n",
        "     \n",
        "  return '' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktrcGqYfTOAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## get finals anchors from the multiple probable hosts\n",
        "\n",
        "def Final_Anchor(txt_path):\n",
        "  filter_probable_list = []\n",
        "  all_lines = []                              \n",
        "  with open (txt_path) as myfile:  \n",
        "      for line in myfile:                   \n",
        "          all_lines.append(line)\n",
        "  # print(len(all_lines))                      \n",
        "  last_line = all_lines[len(all_lines)-1].split('INF|')[1].split('\\n')[0].replace(' ','')\n",
        "  # print(last_line)\n",
        "  avoid_num = ''.join([i for i in last_line if not i.isdigit()])\n",
        "  probable_host_list = avoid_num.split('probable_host:')\n",
        "  probable_host_list = list(filter(None, probable_host_list))\n",
        "\n",
        "  # print(probable_host_list)\n",
        "\n",
        "  for i in probable_host_list:\n",
        "    filter_probable_list.append(get_correct_anchor(i))\n",
        "    filter_probable_list = list(filter(None, filter_probable_list))\n",
        "  return filter_probable_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dv20I0ecT7O7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "c347d00b-1876-481f-b499-56539d8d64e3"
      },
      "source": [
        "#here we are getting all anchors present in a video\n",
        "def get_list_anchors(folder_path):#folder path = where all txt files placed of a show \n",
        "  list_of_anchors = []\n",
        "  list_txt_files = os.listdir(folder_path)\n",
        "  for i in list_txt_files:\n",
        "    txt_file_path = folder_path+'/'+i\n",
        "    # print(Final_Anchor(txt_file_path))\n",
        "    list_of_anchors.append(Final_Anchor(txt_file_path))\n",
        "    print(\"Wait it will take 10-15 min\")\n",
        "  return list_of_anchors"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Ellen DeGeneres'],\n",
              " ['Colleen Williams'],\n",
              " ['Randy Credico', 'Simcha Jacobovici'],\n",
              " [\"Nancy O'Dell\"],\n",
              " ['Colleen Williams', 'Craig Charles'],\n",
              " ['Matt Lauer', 'Katie Couric', 'Russell Baker'],\n",
              " ['Simcha Jacobovici']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5NF0UHmT8iA",
        "colab_type": "text"
      },
      "source": [
        "# **Get Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu3Yw0sKVKYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#this will give us network name either from anchor or show\n",
        "def get_channel(name):\n",
        "  try:\n",
        "    anchor_content = ((((wiki.page(name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. ')))\n",
        "  \n",
        "  except wiki.exceptions.PageError as p:\n",
        "    if PAGE_ERROR in str(p):\n",
        "      return NONE\n",
        "  \n",
        "  except wiki.exceptions.DisambiguationError as e:\n",
        "    list_disambiguation_error = e.options\n",
        "    for anchor_name in list_disambiguation_error:\n",
        "      if anchor_name.find(DISAMBIGUATION_ERROR)!= -1:\n",
        "        return NONE\n",
        "      \n",
        "      else:\n",
        "        anchor_content = ((((wiki.page(anchor_name,auto_suggest=False)).content).replace(' and ',', ').replace('.','. ')))\n",
        "  # print(anchor_content)\n",
        "  Networks_list = NETWORK_TYPES\n",
        "  Network_filter = []\n",
        "    \n",
        "  for network in Networks_list:\n",
        "    # print(word)\n",
        "    if anchor_content.find(network)!=-1:\n",
        "      Network_filter.append(network)\n",
        "  return Network_filter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL6aG2afbxkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_majority_network(anchors_list):\n",
        "  list_of_network = []\n",
        "  for anchorlist in anchors_list:\n",
        "    for anchor in anchorlist:\n",
        "      # print(a)\n",
        "      list_of_network.append(get_channel(anchor))\n",
        "  list_of_network = list(filter(None,list_of_network))\n",
        "  print(list_of_network)\n",
        "\n",
        "  X ={} #dict contains networks and respected votes \n",
        "  for network in list_of_network:\n",
        "    for particular in network:\n",
        "      if particular in X:\n",
        "        X[particular] +=1\n",
        "      else:\n",
        "        X[particular] = 1\n",
        "  print(X)   \n",
        "  Max_votes = max(X.values())\n",
        "  # print(Max_votes)\n",
        "  Majority_Network = [particular for particular, votes in X.items() if votes == Max_votes]\n",
        "  return Majority_Network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0zWPvcqVaq2",
        "colab_type": "text"
      },
      "source": [
        "## **Get Shows**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsIxOfx6Ve3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_nconst(Anchor_Name,Pull_Year):\n",
        "\n",
        "  Read_Titlebasics = pd.read_table(TITLE_BASICS, sep='\\t')\n",
        "  Read_Namebasics = pd.read_table(NAME_BASICS, sep='\\t')\n",
        "  \n",
        "  Read_Namebasics.primaryProfession = Read_Namebasics.primaryProfession.fillna(\"Null\")\n",
        "  filter_year = Read_Namebasics[Read_Namebasics.deathYear >= Pull_Year]\n",
        "  Search_name = filter_year[filter_year.primaryName == Anchor_Name]\n",
        "  # filter_profession = Search_name[Search_name.primaryProfession.str.contains(IMDB_PROFESSION_TYPES)]\n",
        "  # return filter_profession\n",
        "  # return Search_name\n",
        "  # return len(filter_profession.index)\n",
        "  if len(Search_name.index) == 1:\n",
        "    for index, row in Search_name.iterrows():\n",
        "      prepare_list_kft = row.knownForTitles.split(',')\n",
        "      return [row.nconst,prepare_list_kft]\n",
        "\n",
        "  else:\n",
        "     \n",
        "    titleakas = pd.read_table(TITLE_AKAS, sep='\\t')\n",
        "    titleakas.region = titleakas.region.fillna(REGION_TYPE)\n",
        "    filter_region = titleakas[titleakas.region == REGION_TYPE]\n",
        "    filter_region_titleid = filter_region.titleId.tolist()\n",
        "\n",
        "    for index, row in Search_name.iterrows():\n",
        "      prepare_list_kft = row.knownForTitles.split(',')\n",
        "      for i in prepare_list_kft:\n",
        "        take_title = Read_Titlebasics[Read_Titlebasics.tconst == i]\n",
        "        if i in filter_region_titleid:\n",
        "          check_genres = take_title[take_title.genres.str.contains(GENRE_TYPES)]\n",
        "          if len(check_genres.index) == 1:\n",
        "            # print(prepare_list_kft)\n",
        "            # print(check_genres)\n",
        "            return [row.nconst,prepare_list_kft]\n",
        "    return [NONE]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0WpK0WsVzqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_all_shows(nConst):\n",
        "  chunksize = 6*10**6\n",
        "  titleprincipal = pd.read_table(TITLE_PRINCIPALS, chunksize = chunksize)\n",
        "  all_shows = pd.DataFrame(columns=['tconst', 'ordering', 'nconst', 'category', 'job', 'characters'])\n",
        "  #Initializing with the same columns as title.principals\n",
        "  for i,chunk in enumerate((titleprincipal)):\n",
        "    req_shows = chunk[chunk['nconst'] == nConst]\n",
        "    print(' {} shows in chunk {}'.format(len(req_shows), i))\n",
        "    all_shows = all_shows.append(req_shows, ignore_index = True)\n",
        "    # filter_category = all_shows[all_shows.category == 'self']\n",
        "\n",
        "  return all_shows.tconst.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzKlidjXV3R8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get final titles afters union or filter\n",
        "def get_show_names(Anchor_Name,Pull_Year):\n",
        "  store_US_titles = []\n",
        "\n",
        "  store_nconst = get_nconst(Anchor_Name,Pull_Year)\n",
        "\n",
        "  if store_nconst[0] == NONE:\n",
        "    return None\n",
        "\n",
        "  titlecrew = pd.read_table('/content/drive/My Drive/Datasets/title.crew.tsv/data.tsv' ,sep='\\t')\n",
        "  nconst_in_dir = titlecrew[titlecrew.directors.str.contains(store_nconst[0])]\n",
        "  nconst_in_writer = titlecrew[titlecrew.writers.str.contains(store_nconst[0])]\n",
        "  tconst_in_crew = list(set().union(nconst_in_dir.tconst.tolist(),nconst_in_writer.tconst.tolist()))\n",
        "\n",
        "  # Combine_list_1 = list(set().union(store_nconst[1],get_all_shows(store_nconst[0])))\n",
        "  Combine_list_2 = set().union(store_nconst[1],get_all_shows(store_nconst[0]),tconst_in_crew)\n",
        "\n",
        "# Here applying US(region) filter in the shows\n",
        "\n",
        "  titleakas = pd.read_table(TITLE_AKAS, sep='\\t')\n",
        "  titleakas.region = titleakas.region.fillna(REGION_TYPE)\n",
        "  filter_region = titleakas[titleakas.region == REGION_TYPE]\n",
        "  filter_region_titleid = filter_region.titleId.tolist()\n",
        "  for x in Combine_list_2:\n",
        "    if x in filter_region_titleid:\n",
        "      store_US_titles.append(x)\n",
        "\n",
        "  # return store_US_titles\n",
        "  tConsts= set(store_US_titles)\n",
        "\n",
        "  chunksize = 6*10**6\n",
        "\n",
        "  titlebasics = pd.read_table(TITLE_BASICS, chunksize = chunksize)\n",
        "  Show_Names = pd.DataFrame(columns=['tconst', 'titleType', 'primaryTitle', 'originalTitle', 'isAdult', 'startYear', 'endYear', 'runtimeMinutes', 'genres'])\n",
        "  for i,chunk in enumerate(titlebasics):\n",
        "    show_names = chunk[chunk['tconst'].isin(tConsts)]\n",
        "    # show_names = chunk[chunk['tconst'] == 'tt2788528']\n",
        "    print('{} shows in chunk {}'.format(len(show_names), i))\n",
        "    Show_Names = Show_Names.append(show_names, ignore_index = True)\n",
        "    # print(Show_Names)\n",
        "    filter_title_type = Show_Names[Show_Names['titleType']==TITLE_TYPE]\n",
        "    filter_genres = filter_title_type[filter_title_type.genres.str.contains(GENRE_TYPES)]\n",
        "    filter_end_year = filter_genres[filter_genres.endYear >= Pull_Year]\n",
        "    filter_start_year = filter_end_year[filter_end_year.startYear.apply(str) <=Pull_Year]\n",
        "  return filter_start_year "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZC2r0Xq7jSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def final_show(anchor_list):\n",
        "  X = {} #dict contains show name and their votes\n",
        "  for host in anchor_list:\n",
        "    print(host)\n",
        "    store_show_names = get_show_names(host,'2006')\n",
        "    if store_show_names is None:\n",
        "      title = SHOW_NOT_IN_IMDB\n",
        "      if title in X:\n",
        "        X[title] +=1\n",
        "      else:\n",
        "        X[title] = 1\n",
        "    \n",
        "    else:\n",
        "      for title in store_show_names.primaryTitle.tolist():\n",
        "        if title in X:\n",
        "          X[title] +=1\n",
        "        else:\n",
        "          X[title] = 1\n",
        "  if bool(X) is False:\n",
        "    return NONE\n",
        "  else:\n",
        "    Max_votes = max(X.values())\n",
        "    # print(Max_votes)\n",
        "    Majority_Shows = [title for title, votes in X.items() if votes==Max_votes]\n",
        "    if len(Majority_Shows) is 1:\n",
        "      print(Majority_Shows)\n",
        "      # return Majority_Shows\n",
        "    else:\n",
        "      Open_Subtitles = open(\"/content/drive/My Drive/Datasets/Subtitles/2006-01-04_0000_US_00000063_V2_MB7_VHS7_H11_MS.txt3\")\n",
        "      Read_Subtitles = Open_Subtitles.read()\n",
        "\n",
        "      for show in Majority_Shows:\n",
        "        if Read_Subtitles.find(show.upper())!=-1:\n",
        "          print(show)\n",
        "          return show\n",
        "          \n",
        "\n",
        "      for show in Majority_Shows:\n",
        "        for network in get_majority_network(anchor_list):\n",
        "          if show.find(network)!=-1:\n",
        "            print(show)\n",
        "            return show\n",
        "      \n",
        "      # for show in Majority_Shows:\n",
        "      #   call_channel = get_channel(show)\n",
        "      #   if call_channel:\n",
        "      #     for network in call_channel:\n",
        "      #       if network == ('NBC'):\n",
        "      #         return show \n",
        "\n",
        "        # else:\n",
        "        #   return \"network is not available\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHSn1j6AHES1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "final_show(list_of_anchors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scH6vEVSwme4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list_of_anchors = [[\"Heidi Collins\"]]\n",
        "for i in list_of_anchors:\n",
        "  final_show(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6l4Lw9cytYV1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}